---
title: 'Problem Set #5'
author: "Anaya Hall and Christian Miller"
date: "5/2/2018"
output: pdf_document
fontsize: 11pt
geometry: margin=.75in 
---

```{r setup, include=FALSE}

rm(list = ls())
# Setup
knitr::opts_chunk$set(echo = TRUE, cache = T)
# Options
options(stringsAsFactors = F)
# Packages
library(pacman)
p_load(knitr, kableExtra, tidyverse, dplyr, readr, magrittr, ggplot2, readxl, ascii, sandwich, tinytex)

```

# Part 1: Theory
(Optional -- skip for now!)


# Part 2: Instrumental Variables

## Question 1: NLS80
Revisit the model from *Problem Set #3*, now including ability.

$log(wage) = \beta_0 + exper \cdot \beta_1 + tenure \cdot \beta_2 + married \cdot \beta_3 + south \cdot \beta_4 + urban \cdot \beta_5 + black \cdot \beta_6 + educ \cdot \beta_7 + abil \cdot \gamma + \epsilon$

```{r read_data, message=FALSE}

# Read in CSV as data.frame
wage_df <- readr::read_csv("nls80.csv")

# Select only the variables in our model
wage_df %<>% select(lwage, wage, exper, tenure, married, south, urban, black, educ, iq)
```

### (a) Bias of coefficient on education
Derive the bias of $\beta_7$. Show which direction the bias goes in depending on whether the correlation between ability and education is positive or negative.

$ abil = \delta_0 + exper \cdot \delta_1 + tenure \cdot \delta_2 + married \cdot \delta_3 + south \cdot \delta_4 + urban \cdot \delta_5 + black \cdot \delta_6 + educ \cdot \delta_7 + \eta$


$log(wage) = (\beta_0 + \gamma \delta_0) + (\beta_1 + \gamma \delta_1) \cdot educ + (\beta_2 + \gamma \delta_2) \cdot tenure + (\beta_3 + \gamma \delta_3) \cdot married + (\beta_4 + \gamma \delta_4) \cdot south + (\beta_5 + \gamma \delta_5) \cdot urban  + (\beta_6 + \gamma \delta_6) \cdot black + (\beta_7 + \gamma \delta_7) \cdot educ + \gamma \eta + v$

$plim b_7 = \beta_7 + \gamma \delta_7$

Assume that all $\delta$â€™s are zero except for the one on the variable of interest (education)

$plim b_7 = \beta_7 + \gamma \cdot \frac {Cov [ abil , educ ]} {Var [educ]}$

Truth is $\beta_7$ , bias is $\gamma \cdot \frac {Cov [ abil , educ ]} {Var [educ]}$

We expect the sign on $\gamma$ to be positive (higher abiltiy should lead to higher wage), the covariance of ability and education to also be positive (more able people acheive higher levels of education), and, of course, the variance of education is positive.
Thus, the bias will also be *positive*.

### (b) Proxy for ability
Estimate the model above excluding ability, record your parameter estimates, standard errors and $R^2$.

##### - OLS function -
First, let's load our OLS function. 
```{r OLS functions}

# Function to convert tibble, data.frame, or tbl_df to matrix
to_matrix <- function(the_df, vars) {
  # Create a matrix from variables in var
  new_mat <- the_df %>%
    #Select the columns given in 'vars'
    select_(.dots = vars) %>%
    # Convert to matrix
    as.matrix()
  # Return 'new_mat'
  return(new_mat)
}


b_ols <- function(y, X) {
  # Calculate beta hat
  beta_hat <- solve(t(X) %*% X) %*% t(X) %*% y
  # Return beta_hat
  return(beta_hat)
}

ols <- function(data, y_data, X_data, intercept = T, H0 = 0, two_tail = T, alpha = 0.05) {
  # Function setup ----
    # Require the 'dplyr' package
    require(dplyr)
  
  # Create dependent and independent variable matrices ----
    # y matrix
    y <- to_matrix (the_df = data, vars = y_data)
    # X matrix
    X <- to_matrix (the_df = data, vars = X_data)
      # If 'intercept' is TRUE, then add a column of ones
      if (intercept == T) {
      X <- cbind(1,X)
      colnames(X) <- c("intercept", X_data)
      }
 
  # Calculate b, y_hat, and residuals ----
    b <- solve(t(X) %*% X) %*% t(X) %*% y
    y_hat <- X %*% b
    e <- y - y_hat
    
  # Useful -----
    n <- nrow(X) # number of observations
    k <- ncol(X) # number of independent variables
    dof <- n - k # degrees of freedom
    i <- rep(1,n) # column of ones for demeaning matrix
    A <- diag(i) - (1 / n) * i %*% t(i) # demeaning matrix
    y_star <- A %*% y # for SST
    X_star <- A %*% X # for SSM
    SST <- drop(t(y_star) %*% y_star)
    SSM <- drop(t(b) %*% t(X_star) %*% X_star %*% b)
    SSR <- drop(t(e) %*% e)
  
  # Measures of fit and estimated variance ----
    R2uc <- drop((t(y_hat) %*% y_hat)/(t(y) %*% y)) # Uncentered R^2
    R2 <- 1 - SSR/SST # Uncentered R^2
    R2adj <- 1 - (n-1)/dof * (1 - R2) # Adjusted R^2
    AIC <- log(SSR/n) + 2*k/n # AIC
    SIC <- log(SSR/n) + k/n*log(n) # SIC
    s2 <- SSR/dof # s^2
  
  # Measures of fit table ----
    mof_table_df <- data.frame(R2uc, R2, R2adj, SIC, AIC, SSR, s2)
    mof_table_col_names <- c("$R^2_\\text{uc}$", "$R^2$",
                             "$R^2_\\text{adj}$",
                             "SIC", "AIC", "SSR", "$s^2$")
    mof_table <-  mof_table_df %>% knitr::kable(
      row.names = F,
      col.names = mof_table_col_names,
      format.args = list(scientific = F, digits = 4),
      booktabs = T,
      escape = F
    )
  
  # t-test----
    # Standard error
    se <- as.vector(sqrt(s2 * diag(solve(t(X) %*% X))))
    # Vector of _t_ statistics
    t_stats <- (b - H0) / se
    # Calculate the p-values
    if (two_tail == T) {
    p_values <- pt(q = abs(t_stats), df = dof, lower.tail = F) * 2
    } else {
      p_values <- pt(q = abs(t_stats), df = dof, lower.tail = F)
    }
    # Do we (fail to) reject?
    reject <- ifelse(p_values < alpha, reject <- "Reject", reject <- "Fail to Reject")
    
    # Nice table (data.frame) of results
    ttest_df <- data.frame(
      # The rows have the coef. names
      effect = rownames(b),
      # Estimated coefficients
      coef = as.vector(b) %>% round(3),
      # Standard errors
      std_error = as.vector(se) %>% round(4),
      # t statistics
      t_stat = as.vector(t_stats) %>% round(3),
      # p-values
      p_value = as.vector(p_values) %>% round(4),
      # reject null?
      significance = as.character(reject)
      )
  
    ttest_table <-  ttest_df %>% knitr::kable(
      col.names = c("", "Coef.", "S.E.", "t Stat", "p-Value", "Decision"),
      booktabs = T,
      format.args = list(scientific = F),
      escape = F,
      caption = "OLS Results"
    )

  # Data frame for exporting for y, y_hat, X, and e vectors ----
    export_df <- data.frame(y, y_hat, e, X) %>% tbl_df()
    colnames(export_df) <- c("y","y_hat","e",colnames(X))
  
  # Return ----
    return(list(n=n, dof=dof, b=b, se=se, vars=export_df, R2uc=R2uc,R2=R2,
                R2adj=R2adj, AIC=AIC, SIC=SIC, s2=s2, SST=SST, SSR=SSR,
                mof_table=mof_table, ttest=ttest_table))
}

```
\newpage


```{r model1}
model_1 <- ols(wage_df, y_data = "lwage", 
               X_data = c("exper", "tenure", "married", "south", "urban", "black", "educ"))

model_1$ttest

model_1$mof
```

### (c) Include IQ
(c) Estimate the model including IQ as a proxy, record your parameter estimates, standard errors and $R^2$.

```{r model_iq}
model_iq <- ols(wage_df, y_data = "lwage", 
               X_data = c("exper", "tenure", "married", "south", "urban", "black", "educ", "iq"))

model_iq$ttest

model_iq$mof
```

### (d) Returns on education.
*What happens to returns to schooling? Does this result confirm your suspicion of how ability and schooling are expected to be correlated?*

Magnitude of the parameter estimate on education decreased. If IQ **is** a good proxy for ability, this does confirm our suspicion that ability is correlated with education. In the first model, some of the returns on ability (IQ) were mis-attributed to education. In the second model, we correct for this, and see that the parameter estimate on ability is indeed significant.


## Question 2: Recreate results from Card

### (a) Read in data & plot
```{r read_data2, message=FALSE}

# Read in CSV as data.frame
card_df <- readr::read_csv("card.csv")

# Select only the variables in our model
card_df %<>% select(lwage, wage, educ, exper, expersq, black, south, smsa, smsa66, reg661, reg662, reg663, reg664, reg665,reg666, reg667, reg668, nearc4, nearc2)

head(card_df)
```


``` {r plot_series}
ggplot(data = gather(card_df), aes(x = value)) +
  geom_histogram() +
  facet_wrap(~ key, scales = "free") +
  ggtitle("Histograms of Wage Data variables") +
  ylab("Count") +
  xlab("Value") + theme_minimal()
```

### (b) OLS on log(wage)

``` {r lwage_ols}

rhs_vars <- c("educ", "exper", "expersq", "black", "south", "smsa", "smsa66", "reg661", "reg662", "reg663", "reg664", "reg665", "reg666", "reg667", "reg668")

model1 <- ols(card_df, "lwage", rhs_vars)

model1$ttest

```
*CHECK RESULTS AGAINST PAPER*---> I can't find this ?



### (c) Reduced Form
Estimate reduced form equation for *educ* containing all of the explanatory variables and the dummy variable *nearc4*

```{r reduced_form via OLS}

rhs_vars <- c("nearc4", "exper", "expersq", "black", "south", "smsa", "smsa66", "reg661", "reg662", "reg663", "reg664", "reg665", "reg666", "reg667", "reg668")

rf <- ols(card_df, "educ", rhs_vars)

rf$ttest

```


Yes, the partial correlation between *educ* and *nearc4* IS statistically significant!

### (d) Single IV
Estimate the *log(wage)* equation by instrumental variables, using *nearc4* as an instrument for *educ*. 

Compare the 95% confidence interval for the return to educutioan to that obtained from the Least Squares regression above.


```{r singleIV}

# RE-ESTIMATE THIS WITH HETEROSKEDASTICITY ROBUST!! 

Z_vars <- c("nearc4", "exper", "expersq", "black", "south", "smsa", "smsa66", "reg661", "reg662", "reg663", "reg664", "reg665", "reg666", "reg667", "reg668")

# Run OLS
sIV <- ols(card_df, "lwage", Z_vars)

# Parameter Estimates
sIV$ttest

```


``` {r CI_function}

# Create function to use OLS results to calculate 95% Confidence Interval
ci_95 <- function(data) {
  SE <- data$se
  b <- data$b
  
  CI <- matrix(b) %>% 
        cbind(SE) %>% 
            cbind(CI_l = (b - 1.96 * SE), 
                  CI_u = (b + 1.96 * SE))
  colnames(CI) <- c("beta", "SE", "CI Lower", "CI Upper")
  rownames(CI) <- rownames(b)

  CI_table <-  CI[,3:4] #%>% knitr::kable()

  return(CI_table)
  
}

```

``` {r}
# Compare 95% confidence interval for return on education using nearc4 has IV to that of the OLS above (model_1)
ci_95(sIV) %>% knitr::kable(caption = "Return using nearcr as instrument")

ci_95(model1) %>% knitr::kable(caption = "Return on education")

```
Wider confidence intervals using *near4c* as IV than in the original model.

###### ----> heteroskedastic robust?

First bring in functions for Whites Heteroskedasticity robust estimators ???
[below is code from ps3, not quite right though.]
```{r Spherical varcov fxn}

# Function for OLS coef., SE, t-stat, and p-value
vcov_ols <- function(data, y_var, X_vars, intercept = T) {
  # Turn data into matrices
  y <- to_matrix(data, y_var)
  X <- to_matrix(data, X_vars)
  # Add intercept
  if (intercept == T) X <- cbind(1, X)
  # Calculate n and k for degrees of freedom
  n <- nrow(X)
  k <- ncol(X)
  # Estimate coefficients
  b <- b_ols(y, X)
  # Update names
  if (intercept == T) rownames(b)[1] <- "Intercept"
  # Calculate OLS residuals
  e <- y - X %*% b
  # Calculate s^2
  s2 <- (t(e) %*% e) / (n-k)
  s2 %<>% as.numeric()
  # Inverse of X'X
  XX_inv <- solve(t(X) %*% X)
  # Return the results
  return(as.numeric(s2) * XX_inv)
}  
```


```{r Robust varcov fxn}
vcov_white <- function(data, y_var, X_vars, intercept = T) {
  # Turn data into matrices
  y <- to_matrix(data, y_var)
  X <- to_matrix(data, X_vars)
  # Add intercept
  if (intercept == T) X <- cbind(1, X)
  # Calculate n and k for degrees of freedom
  n <- nrow(X)
  k <- ncol(X)
  # Estimate coefficients
  b <- b_ols(y, X)
  # Update names
  if (intercept == T) rownames(b)[1] <- "Intercept"
  # Calculate OLS residuals
  e <- y - X %*% b
  # Inverse of X'X
  XX_inv <- solve(t(X) %*% X)
  # For each row, calculate x_i' x_i e_i^2; then sum
  sigma_hat <- lapply(X = 1:n, FUN = function(i) {
    # Define x_i
    x_i <- matrix(as.vector(X[i,]), nrow = 1)
    # Return x_i' x_i e_i^2
    return(t(x_i) %*% x_i * e[i]^2)
  }) %>% Reduce(f = "+", x = .)
  # Return the results
  return(XX_inv %*% sigma_hat %*% XX_inv)
}
```




### (e) Multiple IV
Use *nearc2* and *nearc4* as instruments for *educ.* 

First, lets build a function for two stage least squares (2SLS or TSLS) - Multiple Instruments
``` {r}
tsls <- function(data, dep_var, endo_vars, exo_vars, i_vars) {
  
  # first stage: Regress the endogenous variables on all exogenous variables and an intercept, save fitted values
  X1 <- unlist(list(c(exo_vars), c(i_vars)))
  x_hat <- ols(data = data, y_data = endo_vars, X_data = X1)$vars$y_hat
  
  namesdf <- names(data)
  data %<>% cbind(x_hat) 
  colnames(data) <- c(namesdf, "x_hat")
  
  # second stage: Rgress the dependent variable on the exogenous x and x_hat
  X2 <- unlist(list(c(exo_vars), "x_hat"))
  
  s2 <- ols(data, y_data = dep_var, X_data = X2)
  
  return(s2)
  
}

exo_vars <- X1 <- c("exper", "expersq", "black", "south", "smsa", 
              "smsa66", "reg661", "reg662", "reg663", "reg664", 
              "reg665", "reg666", "reg667", "reg668")
endo_vars <- c("educ")
i_vars <- c("nearc4", "nearc2")
dep_var <- c("lwage")


#TEST FUNCTION
two_stage <- tsls(data = card_df, dep_var, endo_vars, exo_vars, i_vars)



```

*Comment on the significance of the partial correlations of both instruments in the reduced form.* 
*Show your standard errors from the second stage and compare them to the correct standard errors.*

**FINISH STANDARD ERRORS!!!**


### (f) Hausman test
*Should we worry about endogenaity?*
Conduct a Hausman test for endogeneity of educ. Report your test statistic, critical value and p-value.

Procedure:
1. Regress endogenous var X on instrument(s) Z. save residuals as v_hat
2. Include v_hat in original model
3. test if paramater coefficient on v-hat = 0 (ttest)

*Note: This test is only valid asymptotically (and, of course, is only as good as the instruments used).


``` {r}

hausman <- function(data, dep_var, endo_vars, Z_vars) {
  
  #run ols
  v_hat <- ols(data, endo_vars, Z_vars)$vars$e
  
  namesdf <- names(data)
  data %<>% cbind(v_hat) 
  colnames(data) <- c(namesdf, "v_hat") 
  
  Z2 <- unlist(list(c(Z_vars), "v_hat"))
  
  ht <- ols(data, dep_var, Z2)$ttest
  
  return(ht)
}

Z_vars <- c("exper", "expersq", "black", "south", "smsa", 
              "smsa66", "reg661", "reg662", "reg663", "reg664", 
              "reg665", "reg666", "reg667", "reg668", "nearc4", "nearc2")
endo_vars <- c("educ")
dep_var <- c("lwage")

hausman(card_df, dep_var, endo_vars, Z_vars)

```
The test statistic on v_hat is 21.180, corresponding to a p-value of 0.0000. At a 95% significance level (or even 99% level!) we reject the null hypothesis-- evidence of endogeneity!


