---
title: 'Problem Set #5'
author: "Anaya Hall and Christian Miller"
date: "5/2/2018"
output: pdf_document
fontsize: 11pt
geometry: margin=.75in 
---

```{r setup, include=FALSE}

rm(list = ls())
# Setup
knitr::opts_chunk$set(echo = TRUE, cache = T)
# Options
options(stringsAsFactors = F)
# Packages
library(pacman)
p_load(knitr, kableExtra, tidyverse, dplyr, readr, magrittr, ggplot2, readxl, ascii, sandwich, tinytex)

```

# Part 1: Theory
(Optional -- skip for now!)


# Part 2: Instrumental Variables

## Question 1: NLS80
Revisit the model from *Problem Set #3*, now including ability.

$log(wage) = \beta_0 + exper \cdot \beta_1 + tenure \cdot \beta_2 + married \cdot \beta_3 + south \cdot \beta_4 + urban \cdot \beta_5 + black \cdot \beta_6 + educ \cdot \beta_7 + abil \cdot \gamma + \epsilon$

```{r read_data, message=FALSE}

# Read in CSV as data.frame
wage_df <- readr::read_csv("nls80.csv")

# Select only the variables in our model
wage_df %<>% select(lwage, wage, exper, tenure, married, south, urban, black, educ, iq)
```

### (a) Bias of coefficient on education
Derive the bias of $\beta_7$. Show which direction the bias goes in depending on whether the correlation between ability and education is positive or negative.

$ abil = \delta_0 + exper \cdot \delta_1 + tenure \cdot \delta_2 + married \cdot \delta_3 + south \cdot \delta_4 + urban \cdot \delta_5 + black \cdot \delta_6 + educ \cdot \delta_7 + \eta$


$log(wage) = (\beta_0 + \gamma \delta_0) + (\beta_1 + \gamma \delta_1) \cdot educ + (\beta_2 + \gamma \delta_2) \cdot tenure + (\beta_3 + \gamma \delta_3) \cdot married + (\beta_4 + \gamma \delta_4) \cdot south + (\beta_5 + \gamma \delta_5) \cdot urban  + (\beta_6 + \gamma \delta_6) \cdot black + (\beta_7 + \gamma \delta_7) \cdot educ + \gamma \eta + v$

$plim b_7 = \beta_7 + \gamma \delta_7$

Assume that all $\delta$â€™s are zero except for the one on the variable of interest (education)

$plim b_7 = \beta_7 + \gamma \cdot \frac {Cov [ abil , educ ]} {Var [educ]}$

Truth is $\beta_7$ , bias is $\gamma \cdot \frac {Cov [ abil , educ ]} {Var [educ]}$

We expect the sign on $\gamma$ to be positive (higher abiltiy should lead to higher wage), the covariance of ability and education to also be positive (more able people acheive higher levels of education), and, of course, the variance of education is positive.
Thus, the bias will also be *positive* (biased upward!).

### (b) Proxy for ability
Estimate the model above excluding ability, record your parameter estimates, standard errors and $R^2$.

##### - OLS function -
First, let's load our OLS function. 
```{r OLS functions}

# Function to convert tibble, data.frame, or tbl_df to matrix
to_matrix <- function(the_df, vars) {
  # Create a matrix from variables in var
  new_mat <- the_df %>%
    #Select the columns given in 'vars'
    select_(.dots = vars) %>%
    # Convert to matrix
    as.matrix()
  # Return 'new_mat'
  return(new_mat)
}


b_ols <- function(y, X) {
  # Calculate beta hat
  beta_hat <- solve(t(X) %*% X) %*% t(X) %*% y
  # Return beta_hat
  return(beta_hat)
}

ols <- function(data, y_data, X_data, intercept = T, hetsked = F, H0 = 0, two_tail = T, alpha = 0.05) {
  # Function setup ----
    # Require the 'dplyr' package
    require(dplyr)
  
  # Create dependent and independent variable matrices ----
    # y matrix
    y <- to_matrix (the_df = data, vars = y_data)
    # X matrix
    X <- to_matrix (the_df = data, vars = X_data)
      # If 'intercept' is TRUE, then add a column of ones
      if (intercept == T) {
      X <- cbind(1,X)
      colnames(X) <- c("intercept", X_data)
      }
 
  # Calculate b, y_hat, and residuals ----
    b <- solve(t(X) %*% X) %*% t(X) %*% y
    y_hat <- X %*% b
    e <- y - y_hat
   
    # Inverse of X'X
    XX <- t(X) %*% X
    XX_inv <- solve(t(X) %*% X)
    
    if (hetsked == T) {
      # For each row, calculate x_i' x_i e_i^2; then sum
     sigma_hat <- lapply(X = 1:n, FUN = function(i) {
      # Define x_i
      x_i <- matrix(as.vector(X[i,]), nrow = 1)
      # Return x_i' x_i e_i^2
      return(t(x_i) %*% x_i * e[i]^2)
      }) %>% Reduce(f = "+", x = .) }
    
    if (hetsked == F) sigma_hat <- XX
    
  # Useful -----
    n <- nrow(X) # number of observations
    k <- ncol(X) # number of independent variables
    dof <- n - k # degrees of freedom
    i <- rep(1,n) # column of ones for demeaning matrix
    A <- diag(i) - (1 / n) * i %*% t(i) # demeaning matrix
    y_star <- A %*% y # for SST
    X_star <- A %*% X # for SSM
    SST <- drop(t(y_star) %*% y_star)
    SSM <- drop(t(b) %*% t(X_star) %*% X_star %*% b)
    SSR <- drop(t(e) %*% e)
  
  # Measures of fit and estimated variance ----
    R2uc <- drop((t(y_hat) %*% y_hat)/(t(y) %*% y)) # Uncentered R^2
    R2 <- 1 - SSR/SST # Uncentered R^2
    R2adj <- 1 - (n-1)/dof * (1 - R2) # Adjusted R^2
    AIC <- log(SSR/n) + 2*k/n # AIC
    SIC <- log(SSR/n) + k/n*log(n) # SIC
    s2 <- SSR/dof # s^2
  
  # Measures of fit table ----
    mof_table_df <- data.frame(R2uc, R2, R2adj, SIC, AIC, SSR, s2)
    mof_table_col_names <- c("$R^2_\\text{uc}$", "$R^2$",
                             "$R^2_\\text{adj}$",
                             "SIC", "AIC", "SSR", "$s^2$")
    mof_table <-  mof_table_df %>% knitr::kable(
      row.names = F,
      col.names = mof_table_col_names,
      format.args = list(scientific = F, digits = 4),
      booktabs = T,
      escape = F
    )
  
  # t-test----
    # Standard error
    se <- sqrt(s2 * diag(XX_inv %*% sigma_hat %*% XX_inv)) # Vector of _t_ statistics
    # Vector of _t_ statistics
    t_stats <- (b - H0) / se
    # Calculate the p-values
    if (two_tail == T) {
    p_values <- pt(q = abs(t_stats), df = dof, lower.tail = F) * 2
    } else {
      p_values <- pt(q = abs(t_stats), df = dof, lower.tail = F)
    }
    # Do we (fail to) reject?
    reject <- ifelse(p_values < alpha, reject <- "Reject", reject <- "Fail to Reject")
    
    # Nice table (data.frame) of results
    ttest_df <- data.frame(
      # The rows have the coef. names
      effect = rownames(b),
      # Estimated coefficients
      coef = as.vector(b) %>% round(3),
      # Standard errors
      std_error = as.vector(se) %>% round(4),
      # t statistics
      t_stat = as.vector(t_stats) %>% round(3),
      # p-values
      p_value = as.vector(p_values) %>% round(4),
      # reject null?
      significance = as.character(reject)
      )
  
    ttest_table <-  ttest_df %>% knitr::kable(
      col.names = c("", "Coef.", "S.E.", "t Stat", "p-Value", "Decision"),
      booktabs = T,
      format.args = list(scientific = F),
      escape = F,
      caption = "OLS Results"
    )

  # Data frame for exporting for y, y_hat, X, and e vectors ----
    export_df <- data.frame(y, y_hat, e, X) %>% tbl_df()
    colnames(export_df) <- c("y","y_hat","e",colnames(X))
  
  # Return ----
    return(list(n=n, dof=dof, b=b, se=se, vars=export_df, R2uc=R2uc,R2=R2,
                R2adj=R2adj, AIC=AIC, SIC=SIC, s2=s2, SST=SST, SSR=SSR,
                mof_table=mof_table, ttest=ttest_table))
}

```
\newpage


```{r model1}
model_1 <- ols(wage_df, y_data = "lwage", 
               X_data = c("exper", "tenure", "married", "south", "urban", "black", "educ"))

model_1$ttest

model_1$mof
```

### (c) Include IQ
(c) Estimate the model including IQ as a proxy, record your parameter estimates, standard errors and $R^2$.

```{r model_iq}
model_iq <- ols(wage_df, y_data = "lwage", 
               X_data = c("exper", "tenure", "married", "south", "urban", "black", "educ", "iq"))

model_iq$ttest

model_iq$mof
```

### (d) Returns on education.
*What happens to returns to schooling? Does this result confirm your suspicion of how ability and schooling are expected to be correlated?*

When we include IQ, the magnitude of the parameter estimate for the returns on education decreased, which suggests that we were correct in our guess that the estimate from the first OLS regression was upwardly biased. If IQ **is** a good proxy for ability, this does confirm our suspicion that ability is correlated with education. In the first model, some of the returns on ability (IQ) were mis-attributed to education. In the second model, we correct for this, and see that the parameter estimate on ability is indeed significant.


## Question 2: Recreate results from Card

### (a) Read in data & plot
```{r read_data2, message=FALSE}

# Read in CSV as data.frame
card_df <- readr::read_csv("card.csv")

# Select only the variables in our model
card_df %<>% select(lwage, wage, educ, exper, expersq, black, south, smsa, smsa66, reg661, reg662, reg663, reg664, reg665,reg666, reg667, reg668, nearc4, nearc2)

head(card_df)
```


``` {r plot_series}
ggplot(data = gather(card_df), aes(x = value)) +
  geom_histogram() +
  facet_wrap(~ key, scales = "free") +
  ggtitle("Histograms of Wage Data variables") +
  ylab("Count") +
  xlab("Value") + theme_minimal()
```

### (b) OLS on log(wage)

``` {r lwage_ols}

rhs_vars <- c("educ", "exper", "expersq", "black", "south", "smsa66", "reg661", "reg662", "reg663", "reg664", "reg665", "reg666", "reg667", "reg668", "smsa")

model1 <- ols(card_df, "lwage", rhs_vars)

model1$ttest

```
*CHECK RESULTS AGAINST PAPER*---> I can't find this ?



### (c) Reduced Form
Estimate reduced form equation for *educ* containing all of the explanatory variables and the dummy variable *nearc4*

```{r reduced_form via OLS}

rhs_vars <- c("nearc4", "exper", "expersq", "black", "south", "smsa", "reg661", "reg662", "reg663", "reg664", "reg665", "reg666", "reg667", "reg668", "smsa66")

rf <- ols(card_df, "educ", rhs_vars)

rf$ttest

```


Yes, the partial correlation between *educ* and *nearc4* IS statistically significant!

### (d) Single IV
Estimate the *log(wage)* equation by instrumental variables, using *nearc4* as an instrument for *educ*. 

Compare the 95% confidence interval for the return to educutioan to that obtained from the Least Squares regression above.


```{r IV_function, message = FALSE}

iv <- function(data, y_var, X_vars, Z_vars, intercept = T, hetsked = T) {
    y <- to_matrix (the_df = data, vars = y_vars)
    X <- to_matrix (the_df = data, vars = X_vars)
    Z <- to_matrix (the_df = data, vars = Z_vars)
  
  # Add intercept
  if (intercept == T) X <- cbind(1, X)
  if (intercept == T) Z <- cbind(1, Z)
  # Calculate n and k for degrees of freedom
  n <- nrow(X)
  k <- ncol(X)
  # Estimate coefficients
  b <- solve(t(Z) %*% X) %*% t(Z) %*% y
  # Update names
  if (intercept == T) rownames(b)[1] <- "Intercept" # Calculate OLS residuals
  e <- y - X %*% b
  s2 <- (t(e) %*% e) / (n-k)
  
  # Calculate X_hat
  X_hat <- Z %*% solve(t(Z) %*% Z) %*% t(Z) %*% X 
  # Calculate the inverse of X_hat'X_hat
  XX <- t(X_hat) %*% X_hat
  # Inverse of X'X
  XX_inv <- solve(XX)
  # Calculate the variance-covariance matrix
  if (hetsked == T) {
    sigma_hat <- lapply(X = 1:n, FUN = function(i) {
      # Define x_i
      x_i <- matrix(as.vector(X_hat[i,]), nrow = 1) # Return x_i' x_i e_i^2
      return(t(x_i) %*% x_i * e[i]^2)
    }) %>% Reduce(f = "+", x = .) 
  }
  
  if (hetsked == F) sigma_hat <- XX
  # Calculate the standard error
  se <- sqrt(s2 * diag(XX_inv %*% sigma_hat %*% XX_inv)) # Vector of _t_ statistics
  t_stats <- (b - 0) / se
  # Calculate the p-values
  p_values = pt(q = abs(t_stats), df = n-k, lower.tail = F) * 2 # Names for coefficients
  var_names <- X_vars
  if (intercept == T) var_names <- c("Intercept", var_names)
  
  results <- data.frame(
    effect = rownames(b),
    coef = as.vector(b),
    std_error = as.vector(se),
    t_stat = as.vector(t_stats),
    p_value = as.vector(p_values))
  
  return(results)
}

# RE-ESTIMATE THIS WITH HETEROSKEDASTICITY ROBUST!! 

Z_vars <- c("nearc4", "exper", "expersq", "black", "south", "smsa", "reg661", "reg662", "reg663", "reg664", "reg665", "reg666", "reg667", "reg668", "smsa66")
y_vars <- c("lwage")
X_vars <- c("educ", "exper", "expersq", "black", "south", "smsa", "reg661", "reg662", "reg663", "reg664", "reg665", "reg666", "reg667", "reg668", "smsa66")
# # Run OLS
iv1 <- iv(card_df, y_vars, X_vars, Z_vars, T, T) 

iv1 %>% knitr::kable()


```


``` {r calc CI}
# Compare 95% confidence interval for return on education using nearc4 has IV to that of the OLS above (model_1)

iv_b <- 0.1315038
iv_se <- 0.0210

ols_b <- 0.075
ols_se <- 0.0035

CI <- function(b, se, alpha=1.96) {
  CI <- list((b - alpha*se), (b + alpha*se))
  return(CI)
}

CI(iv_b, iv_se) %>% knitr::kable(caption = "Return using nearcr as instrument")

CI(ols_b, ols_se) %>% knitr::kable(caption = "Return on education")


```
Wider confidence intervals using *near4c* as IV than in the original model. The 95% confidence interval using the instrument is [0.0903, 0.1727], while from OLS it was [0.0681,0.0819]. 


First bring in functions for Whites Heteroskedasticity robust estimators ???

```{r Robust varcov fxn}
vcov_white <- function(data, y_var, X_vars, intercept = T) {
  # Turn data into matrices
  y <- to_matrix(data, y_var)
  X <- to_matrix(data, X_vars)
  # Add intercept
  if (intercept == T) X <- cbind(1, X)
  # Calculate n and k for degrees of freedom
  n <- nrow(X)
  k <- ncol(X)
  # Estimate coefficients
  b <- b_ols(y, X)
  # Update names
  if (intercept == T) rownames(b)[1] <- "Intercept"
  # Calculate OLS residuals
  e <- y - X %*% b
  # Inverse of X'X
  XX_inv <- solve(t(X) %*% X)
  # For each row, calculate x_i' x_i e_i^2; then sum
  sigma_hat <- lapply(X = 1:n, FUN = function(i) {
    # Define x_i
    x_i <- matrix(as.vector(X[i,]), nrow = 1)
    # Return x_i' x_i e_i^2
    return(t(x_i) %*% x_i * e[i]^2)
  }) %>% Reduce(f = "+", x = .)
  # Return the results
  return(XX_inv %*% sigma_hat %*% XX_inv)
}
```




### (e) Multiple IV
Use *nearc2* and *nearc4* as instruments for *educ.* 

First, lets build a function for two stage least squares (2SLS or TSLS) - Multiple Instruments
``` {r eds 2sls function}
b_2sls <- function(data, y_var, X_vars, Z_vars, intercept = T) {
  # Turn data into matrices
  y <- to_matrix(data, y_var)
  X <- to_matrix(data, X_vars)
  Z <- to_matrix(data, Z_vars)
  # Add intercept
  if (intercept == T) X <- cbind(1, X)
  if (intercept == T) Z <- cbind(1, Z)
  # Estimate the first stage
  b_stage1 <- solve(t(Z) %*% Z) %*% t(Z) %*% X
  # Fit the first stage values
  X_hat <- Z %*% b_stage1
  # Estimate the second stage
  b_stage2 <- solve(t(X_hat) %*% X_hat) %*% t(X_hat) %*% y
  # Update names
  if (intercept == T) rownames(b_stage2)[1] <- "Intercept"
  # Return beta_hat
  return(b_stage2)
}


````


``` {r}
tsls <- function(data, y_vars, X_vars, Z_vars, intercept = T, hetsked = F) {
  
  # Turn data into matrices
  y <- to_matrix(data, y_vars)
  X <- to_matrix(data, X_vars)
  Z <- to_matrix(data, Z_vars)
  # Calculate n and k for degrees of freedom
  n <- nrow(X)
  k <- ncol(X)
  # Add intercept
  if (intercept == T) X <- cbind(1, X)
  if (intercept == T) Z <- cbind(1, Z)
  # Calculate P_Z
  P_Z <- Z %*% solve(t(Z) %*% Z) %*% t(Z)
  # Calculate b_2sls
  b <- solve(t(X) %*% P_Z %*% X) %*% t(X) %*% P_Z %*% y
  # Calculate OLS residuals
  e <- y - X %*% b
  # Calculate s^2
  s2 <- (t(e) %*% e) / (n - k)
  s2 %<>% as.numeric()
  # Inverse of X' Pz X
  XX_inv <- solve(t(X) %*% P_Z %*% X)
  # Standard error
  se <- sqrt(s2 * diag(XX_inv))
  # Vector of _t_ statistics
  t_stats <- (b - 0) / se
  # Calculate the p-values
  p_values = pt(q = abs(t_stats), df = n-k, lower.tail = F) * 2
  # Update names
  if (intercept == T) rownames(b)[1] <- "Intercept"
  # Nice table (data.frame) of results
  results <- data.frame(
    # The rows have the coef. names
    effect = rownames(b),
    # Estimated coefficients
    coef = as.vector(b),
    # Standard errors
    std_error = as.vector(se),
    # t statistics
    t_stat = as.vector(t_stats),
    # p-values
    p_value = as.vector(p_values)
    )
  # Return the results
  return(results)
  
}

Z_vars <- c("nearc4", "nearc2", "exper", "expersq", "black", "south", "smsa", "reg661", "reg662", "reg663", "reg664", "reg665", "reg666", "reg667", "reg668", "smsa66")
y_vars <- c("lwage")
X_vars <- c("educ", "exper", "expersq", "black", "south", "smsa", "reg661", "reg662", "reg663", "reg664", "reg665", "reg666", "reg667", "reg668", "smsa66")

#TEST FUNCTION
two_stage <- tsls(data = card_df, y_vars, X_vars, Z_vars, T, F)

two_stage

```

*Comment on the significance of the partial correlations of both instruments in the reduced form.* 
*Show your standard errors from the second stage and compare them to the correct standard errors.*

---> what's the difference between second stage and 'correct' 2sls standard errors?


### (f) Hausman test
*Should we worry about endogenaity?*
Conduct a Hausman test for endogeneity of educ. Report your test statistic, critical value and p-value.

Procedure:
1. Regress endogenous var X on instrument(s) Z. save residuals as v_hat
2. Include v_hat in original model
3. test if paramater coefficient on v-hat = 0 (ttest)

*Note: This test is only valid asymptotically (and, of course, is only as good as the instruments used).


``` {r}

hausman <- function(data, dep_var, endo_vars, Z_vars) {
  
  #run ols
  v_hat <- ols(data, endo_vars, Z_vars, T, F)$vars$e
  
  namesdf <- names(data)
  data %<>% cbind(v_hat) 
  colnames(data) <- c(namesdf, "v_hat") 
  
  Z2 <- unlist(list(c(Z_vars), c(endo_vars), "v_hat"))
  
  ht <- ols(data, dep_var, Z2, T, F)$ttest
  
  return(ht)
}

Z_vars <- c("exper", "expersq", "black", "south", "smsa", 
              "smsa66", "reg661", "reg662", "reg663", "reg664", 
              "reg665", "reg666", "reg667", "reg668", "nearc4", "nearc2")
endo_vars <- c("educ")
dep_var <- c("lwage")

hausman(card_df, dep_var, endo_vars, Z_vars)

### FORGET THE FUNCTION AND JUST TRY TO SOLVE!


```
The test statistic on v_hat is 21.180, corresponding to a p-value of 0.0000. At a 95% significance level (or even 99% level!) we reject the null hypothesis-- evidence of endogeneity!


